{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, StringType\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium import LinearColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Initialize Spark session\n",
    "######################################\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GridProcessing\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Define the grid cell size\n",
    "######################################\n",
    "\n",
    "lat_start, lon_start = 45.687757, 4.744455\n",
    "\n",
    "lat_step = 0.0030  # 500m en latitude\n",
    "lon_step = 0.0045  # 500m en longitude\n",
    "\n",
    "rows = 40 # 20km * 500m\n",
    "cols = 60 # 20km * 500m\n",
    "\n",
    "# Create a list of all grid cells\n",
    "grid_cells = []\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        lat1 = lat_start + i * lat_step\n",
    "        lon1 = lon_start + j * lon_step\n",
    "        lat2 = lat1 + lat_step\n",
    "        lon2 = lon1 + lon_step\n",
    "        lat_center = (lat1 + lat2) / 2\n",
    "        lon_center = (lon1 + lon2) / 2\n",
    "        \n",
    "        grid_cells.append((i * cols + j, lat1, lon1, lat2, lon2, lat_center, lon_center))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "grid_cells_df = spark.createDataFrame(grid_cells, [\"id\", \"lat1\", \"lon1\", \"lat2\", \"lon2\", \"lat_center\", \"lon_center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Fonciere data\n",
    "######################################\n",
    "\n",
    "# Define the schema and read the data\n",
    "fonciere_schema = StructType([\n",
    "    StructField(\"VALEUR_FONCIERE\", FloatType(), False), \n",
    "    StructField(\"CODE_POSTAL\", StringType(), False),\n",
    "    StructField(\"NOM_COMMUNE\", StringType(), False), \n",
    "    StructField(\"TYPE_LOCAL\", StringType(), False),\n",
    "    StructField(\"SURFACE_REELLE_BATI\", FloatType(), False), \n",
    "    StructField(\"SURFACE_TERRAIN\", FloatType(), False), \n",
    "    StructField(\"LONGITUDE\", FloatType(), False),\n",
    "    StructField(\"LATITUDE\", FloatType(), False), \n",
    "    StructField(\"prix_du_m_carre\", FloatType(), False)\n",
    "])\n",
    "\n",
    "fonciere_df = spark.read.option(\"header\", True).schema(fonciere_schema).csv(\"./cleaned/valeurs_foncieres.csv\")\n",
    "\n",
    "# Join the grid cells DataFrame with fonciere_df based on latitude and longitude conditions\n",
    "joined_df = fonciere_df.crossJoin(grid_cells_df).where(\n",
    "    (fonciere_df.LATITUDE >= grid_cells_df.lat1) & (fonciere_df.LATITUDE <= grid_cells_df.lat2) &\n",
    "    (fonciere_df.LONGITUDE >= grid_cells_df.lon1) & (fonciere_df.LONGITUDE <= grid_cells_df.lon2)\n",
    ")\n",
    "\n",
    "# Group by grid cell ID and calculate the mean and max values for prix_du_m_carre\n",
    "fonciere_joined_df = joined_df.groupBy(\"id\").agg(\n",
    "    F.mean(\"prix_du_m_carre\").alias(\"mean_prix_du_m_carre\"),\n",
    "    F.max(\"prix_du_m_carre\").alias(\"max_prix_du_m_carre\")\n",
    ").fillna(\n",
    "    0, subset=[\"mean_prix_du_m_carre\", \"max_prix_du_m_carre\"]\n",
    ").withColumn(\n",
    "    \"score_prix\",\n",
    "    F.when((F.col(\"mean_prix_du_m_carre\") == 0) & (F.col(\"max_prix_du_m_carre\") == 0), 0)\n",
    "    .otherwise((1 - (F.col(\"mean_prix_du_m_carre\") / F.col(\"max_prix_du_m_carre\")))) * 100\n",
    ")\n",
    "\n",
    "fonciere_joined_df = grid_cells_df.join(fonciere_joined_df, on=\"id\", how=\"left\").orderBy(\"score_prix\")\n",
    "fonciere_joined_df = fonciere_joined_df.fillna(0)\n",
    "fonciere_joined_df = fonciere_joined_df.select(\"id\", \"score_prix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Hopitaux data\n",
    "######################################\n",
    "\n",
    "hopitaux_schema = StructType([\n",
    " StructField(\"NOM\", StringType(), False),\n",
    " StructField(\"LATITUDE\", FloatType(), False),\n",
    " StructField(\"LONGITUDE\", FloatType(), False)\n",
    "])\n",
    "\n",
    "hopitaux_df = spark.read.option(\"header\", True).option(\"sep\", \";\").schema(hopitaux_schema).csv(\"./cleaned/adr_voie_lieu.adrhopital.csv\")\n",
    "\n",
    "# Broadcast is more efficient than a crossJoin\n",
    "grid_cells_df_broadcast = F.broadcast(grid_cells_df)\n",
    "\n",
    "hopitaux_joined_df = (\n",
    "    hopitaux_df\n",
    "    .join(grid_cells_df_broadcast, \n",
    "          (hopitaux_df.LATITUDE >= grid_cells_df_broadcast.lat1) & \n",
    "          (hopitaux_df.LATITUDE <= grid_cells_df_broadcast.lat2) &\n",
    "          (hopitaux_df.LONGITUDE >= grid_cells_df_broadcast.lon1) & \n",
    "          (hopitaux_df.LONGITUDE <= grid_cells_df_broadcast.lon2))\n",
    "    .groupBy(\"id\")\n",
    "    .count()\n",
    "    .withColumn(\"score_hopitaux\", (F.col(\"count\") / hopitaux_df.count()) * 1000)\n",
    "    .select(\"id\", \"score_hopitaux\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# PAM data\n",
    "######################################\n",
    "\n",
    "docteurs = StructType([\n",
    " StructField(\"ID\", StringType(), False),\n",
    " StructField(\"PROFESSION\", StringType(), False),\n",
    " StructField(\"COMMUNE\", StringType(), False),\n",
    " StructField(\"NOM\", StringType(), False),\n",
    " StructField(\"LATITUDE\", FloatType(), False),\n",
    " StructField(\"LONGITUDE\", FloatType(), False)\n",
    "])\n",
    "\n",
    "docteurs_df = spark.read.option(\"header\", True).schema(docteurs).csv(\"./cleaned/pam-doc.csv\")\n",
    "\n",
    "# Calculer le nombre de professions distinctes\n",
    "distinct_profession_count = docteurs_df.select(\"PROFESSION\").distinct().count()\n",
    "\n",
    "practitiens_final = (\n",
    "    docteurs_df\n",
    "    .crossJoin(grid_cells_df_broadcast.withColumnRenamed(\"id\", \"zone_id\"))\n",
    "    .where(\n",
    "        (F.col(\"LATITUDE\") >= F.col(\"lat1\")) & (F.col(\"LATITUDE\") <=F.col(\"lat2\")) &\n",
    "        (F.col(\"LONGITUDE\") >= F.col(\"lon1\")) & (F.col(\"LONGITUDE\") <= F.col(\"lon2\"))\n",
    "    )\n",
    "    # Calculer le ratio de praticiens par profession\n",
    "    .groupBy(\"zone_id\", \"PROFESSION\")\n",
    "    .agg(F.count(\"*\").alias(\"filtered_count\"))\n",
    "    # totaux par profession\n",
    "    .join(\n",
    "        docteurs_df.groupBy(\"PROFESSION\").agg(F.count(\"*\").alias(\"profession_count\")),\n",
    "        on=\"PROFESSION\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # joindre le ratio de praticiens par profession et les totaux par profession\n",
    "    .withColumn(\n",
    "        \"ratio\",\n",
    "        F.when(F.col(\"profession_count\") > 0, F.col(\"filtered_count\") / F.col(\"profession_count\")).otherwise(0)\n",
    "    )\n",
    "    .groupBy(\"zone_id\")\n",
    "    .agg(F.sum(\"ratio\").alias(\"sum_ratios\"))\n",
    "    # nombre de profession / zone\n",
    "    .withColumn(\n",
    "        \"score_practicien\",\n",
    "        F.when(F.col(\"sum_ratios\").isNull(), F.lit(0))\n",
    "        .otherwise(F.col(\"sum_ratios\") / F.lit(distinct_profession_count)) * 100\n",
    "    )\n",
    "    .join(grid_cells_df_broadcast, grid_cells_df_broadcast.id == F.col(\"zone_id\"), how=\"left\")\n",
    "    .drop(\"sum_ratios\", \"id\")\n",
    "    .orderBy(F.col(\"score_practicien\").desc())\n",
    "    .select(\n",
    "        \"zone_id\", \"score_practicien\"\n",
    "    )\n",
    "    .fillna(0, subset=[\"score_practicien\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Subway stations data\n",
    "######################################\n",
    "\n",
    "# Charger les stations de métro\n",
    "stations_schema = StructType([\n",
    "    StructField(\"nom\", StringType(), False),\n",
    "    StructField(\"longitude\", FloatType(), False),\n",
    "    StructField(\"latitude\", FloatType(), False),\n",
    "    StructField(\"nb\", FloatType(), False)\n",
    "])\n",
    "\n",
    "df_stations = spark.read.csv('cleaned/subway_stations_with_nb.csv', schema=stations_schema, header=True, sep=';')\n",
    "\n",
    "# Supprimer les doublons en gardant une seule instance par nom de station\n",
    "df_stations_distinct = df_stations.groupBy(\"nom\").agg(\n",
    "    F.first(\"longitude\").alias(\"longitude\"),\n",
    "    F.first(\"latitude\").alias(\"latitude\"),\n",
    "    F.first(\"nb\").alias(\"nb\")\n",
    ")\n",
    "\n",
    "# Diffuser la grille pour optimiser la jointure\n",
    "grid_cells_df_broadcast = F.broadcast(grid_cells_df)\n",
    "\n",
    "# Joindre les stations de métro à la grille\n",
    "subway_joined_df = (\n",
    "    df_stations_distinct\n",
    "    .join(grid_cells_df_broadcast,\n",
    "          (df_stations_distinct.latitude >= grid_cells_df_broadcast.lat1) &\n",
    "          (df_stations_distinct.latitude <= grid_cells_df_broadcast.lat2) &\n",
    "          (df_stations_distinct.longitude >= grid_cells_df_broadcast.lon1) &\n",
    "          (df_stations_distinct.longitude <= grid_cells_df_broadcast.lon2),\n",
    "          how=\"inner\")\n",
    "    .groupBy(\"id\")\n",
    "    .agg(F.sum(\"nb\").alias(\"subway_count\"))  \n",
    "    .withColumn(\"score_metro\", (F.col(\"subway_count\") / df_stations_distinct.count()) * 100)\n",
    "    .select(\"id\", \"score_metro\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Parking data\n",
    "######################################\n",
    "\n",
    "parking_schema = StructType([\n",
    "     StructField(\"NOM\", StringType(), False),\n",
    "     StructField(\"HANDICAP\", StringType(), False), \n",
    "     StructField(\"CAPACITE_TOTALE\", StringType(), False), \n",
    "     StructField(\"CAPACITE_PERMANENTE\", StringType(), False), \n",
    "     StructField(\"ACCUEIL_TEMPORAIRE\", StringType(), False), \n",
    "     StructField(\"LONGITUDE\", FloatType(), False),\n",
    "     StructField(\"LATITUDE\", FloatType(), False)\n",
    "])\n",
    "\n",
    "parking_df = spark.read.option(\"header\", True).schema(parking_schema).csv(\"./cleaned/places_parking.csv\")\n",
    "\n",
    "# Get the total parking capacity\n",
    "total_parking_capacity = parking_df.select(F.sum(parking_df.CAPACITE_TOTALE.cast('int')).alias('total_capacity')).collect()[0]['total_capacity']\n",
    "\n",
    "parking_final_df = (\n",
    "    parking_df.join(grid_cells_df, (\n",
    "        (parking_df.LATITUDE >= grid_cells_df.lat1) & \n",
    "        (parking_df.LATITUDE <= grid_cells_df.lat2) &\n",
    "        (parking_df.LONGITUDE >= grid_cells_df.lon1) & \n",
    "        (parking_df.LONGITUDE <= grid_cells_df.lon2)\n",
    "    ), how=\"right\")\n",
    "    .groupBy(\"id\")\n",
    "    .agg(\n",
    "        F.sum(parking_df.CAPACITE_TOTALE.cast('int')).alias('total_capacity_per_zone')\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"score_parking\",\n",
    "        F.when(F.col(\"total_capacity_per_zone\").isNotNull(), F.col(\"total_capacity_per_zone\") / total_parking_capacity)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    .select(\n",
    "        \"id\", \"score_parking\"\n",
    "    )\n",
    "    .fillna(0, subset=[\"score_parking\"])\n",
    "    .orderBy(F.col(\"score_parking\").desc()) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " id               | 0                  \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 1------------------------------\n",
      " id               | 1                  \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 2------------------------------\n",
      " id               | 2                  \n",
      " score_prix       | 14.043079647160761 \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 3------------------------------\n",
      " id               | 3                  \n",
      " score_prix       | 8.050102780801105  \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 4------------------------------\n",
      " id               | 4                  \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 5------------------------------\n",
      " id               | 5                  \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 6------------------------------\n",
      " id               | 6                  \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 7------------------------------\n",
      " id               | 7                  \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 8------------------------------\n",
      " id               | 8                  \n",
      " score_prix       | 33.10827871768976  \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 9------------------------------\n",
      " id               | 9                  \n",
      " score_prix       | 26.83499921516602  \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 10-----------------------------\n",
      " id               | 10                 \n",
      " score_prix       | 6.0025128431704715 \n",
      " score_hopitaux   | 19.607843137254903 \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 11-----------------------------\n",
      " id               | 11                 \n",
      " score_prix       | 50.63829188007598  \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 12-----------------------------\n",
      " id               | 12                 \n",
      " score_prix       | 31.068568853563093 \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 13-----------------------------\n",
      " id               | 13                 \n",
      " score_prix       | 27.786812817890027 \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 14-----------------------------\n",
      " id               | 14                 \n",
      " score_prix       | 18.603898000121898 \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 15-----------------------------\n",
      " id               | 15                 \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 16-----------------------------\n",
      " id               | 16                 \n",
      " score_prix       | 26.809149929632493 \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 17-----------------------------\n",
      " id               | 17                 \n",
      " score_prix       | 38.149308011135105 \n",
      " score_hopitaux   | 19.607843137254903 \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 18-----------------------------\n",
      " id               | 18                 \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "-RECORD 19-----------------------------\n",
      " id               | 19                 \n",
      " score_prix       | 0.0                \n",
      " score_hopitaux   | 0.0                \n",
      " score_practicien | 0.0                \n",
      " score_metro      | 0.0                \n",
      " score_parking    | 0.0                \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Final part\n",
    "######################################\n",
    "\n",
    "# important de partir de fonciere_joined_df pour ne pas perdre les cellules sans hopitaux\n",
    "final_df = fonciere_joined_df.join(hopitaux_joined_df, on=\"id\", how=\"left\")\n",
    "final_df = final_df.join(practitiens_final, fonciere_joined_df.id == practitiens_final.zone_id, how=\"left\").drop(\"zone_id\")\n",
    "final_df = final_df.join(subway_joined_df, on=\"id\", how=\"left\").fillna(0)\n",
    "final_df = final_df.join(parking_final_df, on=\"id\", how=\"left\")\n",
    "\n",
    "final_df = final_df.fillna(0)\n",
    "final_df.show(truncate=False, vertical=True)\n",
    "\n",
    "assert final_df.count() == grid_cells_df.count(), \"Both datasets must contains the same number of items\"\n",
    "\n",
    "final_df_pandas = final_df.toPandas()\n",
    "final_df_pandas.to_csv('grid_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pyspark-env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lat1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m colormap \u001b[38;5;241m=\u001b[39m LinearColormap([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m], vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m final_df_pandas\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 15\u001b[0m     lat1, lon1, lat2, lon2 \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat2\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m     square_coords \u001b[38;5;241m=\u001b[39m [(lat1, lon1), (lat1, lon2), (lat2, lon2), (lat2, lon1)]\n\u001b[1;32m     19\u001b[0m     s_prix \u001b[38;5;241m=\u001b[39m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_prix\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark-env/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark-env/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark-env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lat1'"
     ]
    }
   ],
   "source": [
    "lat, lon = 45.751719, 4.880243\n",
    "zoom_start = 12\n",
    "\n",
    "tiles = 'https://tile.jawg.io/jawg-lagoon/{z}/{x}/{y}{r}.png'\n",
    "attr = (\n",
    "    '&copy; <a href=\"https://jawg.io\" title=\"Tiles Courtesy of Jawg Maps\" target=\"_blank\">Jawg Maps</a> &copy; '\n",
    "    '<a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors'\n",
    ")\n",
    "\n",
    "m = folium.Map(location=[lat, lon], tiles=tiles, attr=attr, zoom_start=zoom_start)\n",
    "\n",
    "colormap = LinearColormap(['blue', 'green', 'yellow', 'orange', 'red'], vmin=0, vmax=100)\n",
    "\n",
    "for index, row in final_df_pandas.iterrows():\n",
    "    lat1, lon1, lat2, lon2 = row['lat1'], row['lon1'], row['lat2'], row['lon2']\n",
    "    \n",
    "    square_coords = [(lat1, lon1), (lat1, lon2), (lat2, lon2), (lat2, lon1)]\n",
    "\n",
    "    s_prix = (row['score_prix'])\n",
    "    s_hopitaux = (row['score_hopitaux'])\n",
    "    s_pam = (row['score_practiciencalcul_practicien'])\n",
    "    s_metro = (row['score_metro'])\n",
    "    s_parking = (row['score_parking'])\n",
    "    \n",
    "    color = colormap(s_prix)\n",
    "\n",
    "    folium.Polygon(\n",
    "        locations=square_coords,\n",
    "        color=None,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-env]",
   "language": "python",
   "name": "conda-env-pyspark-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
